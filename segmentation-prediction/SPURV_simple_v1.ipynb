{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=7, micro=6, releaselevel='final', serial=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/audun/anaconda3/envs/segmentation-prediction/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/audun/anaconda3/envs/segmentation-prediction/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/audun/anaconda3/envs/segmentation-prediction/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/audun/anaconda3/envs/segmentation-prediction/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/audun/anaconda3/envs/segmentation-prediction/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/audun/anaconda3/envs/segmentation-prediction/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "4.2.0\n",
      "segmentation-prediction\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version_info)\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import configparser\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import itertools\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "#from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "#from tensorflow.keras.models import Model\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "#from tensorflow.keras.layers import Input, Flatten, Dense, Cropping2D, Conv2D, concatenate, TimeDistributed, CuDNNLSTM\n",
    "#from tensorflow.keras.utils import Sequence\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Flatten, Dense, Cropping2D, Conv2D, concatenate, TimeDistributed, CuDNNLSTM\n",
    "from keras.utils import Sequence\n",
    "import random\n",
    "from keras_segmentation.predict import model_from_checkpoint_path\n",
    "from keras.backend import tf\n",
    "\n",
    "checkpoints_path = \"/home/audun/master-thesis-code/training/psp_checkpoints_best/pspnet_50_three\"\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "import scipy\n",
    "# import pydotplus\n",
    "\n",
    "print(tf.__version__)\n",
    "print(cv2.__version__)\n",
    "print(os.environ[\"CONDA_DEFAULT_ENV\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "hlc_one_hot = { 0: [1,0,0], 1:[0,1,0], 2:[0,0,1] }\n",
    "#hlc_one_hot = { 1: [1,0,0,0], 2:[0,1,0,0], 3:[0,0,1,0], 4:[0,0,0,1]}\n",
    "\n",
    "def flatten_list(items_list):\n",
    "    \"\"\" Removes the episode dimension from a list of data points \"\"\"\n",
    "    ret = []\n",
    "    for items in items_list:\n",
    "        for item in items:\n",
    "            ret.append(item)        \n",
    "    return ret      \n",
    "\n",
    "# Custom loss function \n",
    "def weighted_mse(y_true, y_pred, weight_mask):\n",
    "    \"\"\" \n",
    "    Custom loss fucntion, different weigted loss to steer/throttle \n",
    "    Used when all outputs is evaluated by same loss function \n",
    "    \"\"\"\n",
    "    return K.mean(K.square(y_pred - y_true)*weight_mask, axis=-1)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    \"\"\" Custom loss function, RMSE \"\"\"\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\" Custom loss function, RMSE \"\"\"\n",
    "    return K.mean(K.square(y_pred - y_true))\n",
    "\n",
    "\n",
    "\n",
    "def steer_loss():\n",
    "    \"\"\" Loss function for steering, RMSE \"\"\"\n",
    "    def custom(y_true, y_pred):\n",
    "        return root_mean_squared_error(y_true, y_pred)\n",
    "    return custom\n",
    "\n",
    "def get_hlc_one_hot(hlc):\n",
    "    \"\"\" One-hot encode HLC values \"\"\"\n",
    "    return hlc_one_hot[hlc]\n",
    "\n",
    "def sine_encode(angle):\n",
    "    \"\"\" Encode steering angle as sine wave \"\"\"\n",
    "    angle_max = 1\n",
    "    N = 10\n",
    "    ret = []\n",
    "    \n",
    "    for i in range(1,N+1,1):\n",
    "        Y = np.sin(((2*np.pi*(i-1))/(N-1))-((angle*np.pi)/(2*angle_max)))\n",
    "        ret.append(Y)\n",
    "    \n",
    "    return np.array(ret)\n",
    "\n",
    "def create_input_dict():\n",
    "    return {\n",
    "        \"forward_imgs\": [],\n",
    "        \"hlcs\" : [],\n",
    "    }\n",
    "\n",
    "def create_target_dict():\n",
    "    return {\n",
    "        \"steer\" : [],\n",
    "        \"throttle\": [],\n",
    "    }\n",
    "\n",
    "def split_dict(dictionary, split_pos):\n",
    "    \"\"\" Split data into training and validation \"\"\"\n",
    "    train_dict = {}\n",
    "    val_dict = {}\n",
    "    \n",
    "    for key in dictionary:\n",
    "        train_dict[key] = dictionary[key][:split_pos]\n",
    "        val_dict[key] = dictionary[key][split_pos:]\n",
    "    \n",
    "    return train_dict, val_dict\n",
    "\"\"\"\n",
    "def adjust_hlcs(hlcs, info_signals):\n",
    "    Randomly adjust HLC backwards, \n",
    "    such that the HLC data is given before the car is in the intersection \n",
    "    \n",
    "    # Iterate over all episodes \n",
    "    for e in range(len(hlcs)): \n",
    "        active_change_hlc = None\n",
    "        changed = 0\n",
    "        adjust_num = random.randint(20, 40)\n",
    "        # Iterate over all hlcs in episode \n",
    "        for i in range(len(hlcs[e])-2,-1, -1):\n",
    "            next_hlc = np.argmax(hlcs[e][i+1]) + 1 \n",
    "            current_hlc = np.argmax(hlcs[e][i]) +1 \n",
    "            if active_change_hlc == False:\n",
    "                active_change_hlc = None\n",
    "                continue\n",
    "\n",
    "            if active_change_hlc is None:\n",
    "                if next_hlc == 1 or next_hlc == 2 or next_hlc == 3:                     \n",
    "                    if current_hlc == 4: \n",
    "                        active_change_hlc = hlcs[e][i+1]\n",
    "            if active_change_hlc is not None: \n",
    "                hlcs[e][i] = active_change_hlc\n",
    "                if info_signals[e][i][2] != 0: \n",
    "                    changed += 1\n",
    "                if changed == adjust_num:\n",
    "                    active_change_hlc = False\n",
    "                    changed = 0 \n",
    "    return hlcs \n",
    "\"\"\"\n",
    "#### BALANCING DATA - Helper functions #### \n",
    "def get_hlc_dist_label(hlc): \n",
    "    \" Get distributions of HLC in data \"\n",
    "    if hlc == 0: \n",
    "        return \"left\"\n",
    "    elif hlc == 2:\n",
    "        return \"right\"\n",
    "    elif hlc == 1:\n",
    "        return \"straight\"\n",
    "    \n",
    "def get_speed_dist_label(speed): \n",
    "    \"\"\" Get distributions of speed in data \"\"\"\n",
    "\n",
    "    if speed < 0.001: \n",
    "        return \"low_speed\"\n",
    "    else: \n",
    "        return \"high_speed\"\n",
    "    \n",
    "def get_speed_limit_dist_label(speed_limit): \n",
    "    \"\"\" Get distributions of speed limits in data \"\"\"\n",
    "\n",
    "    speed_limit = round(speed_limit, 1)\n",
    "    if speed_limit == 0.3: \n",
    "        return \"30km/h\"\n",
    "    elif speed_limit == 0.6: \n",
    "        return \"60km/h\"\n",
    "    elif speed_limit == 0.9: \n",
    "        return \"90km/h\"\n",
    "    \n",
    "def get_percentage_dist(dist): \n",
    "    \"\"\" Get distribution of data in percentage \"\"\"\n",
    "    N = dist[\"traffic_light\"][\"red\"] + dist[\"traffic_light\"][\"green\"]\n",
    "    for dist_key in dist:\n",
    "        for key, dist_val in dist[dist_key].items(): \n",
    "            dist[dist_key][key] = dist_val/N \n",
    "    return dist \n",
    "\n",
    "def get_drop_num(tot_num, num, keep_fraction):\n",
    "    \"\"\" Calculates how many datasamples one should drop to get the right amount of data samples \"\"\"\n",
    "    return int((num - keep_fraction*tot_num)/(1-keep_fraction))\n",
    "\n",
    "\n",
    "def shuffle_data(inputs_flat, targets_flat):\n",
    "    # Shuffle\n",
    "    indices = np.arange(len(inputs_flat[\"forward_imgs\"]))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for key in inputs_flat: \n",
    "        inputs_flat[key] = np.array(inputs_flat[key])[indices]\n",
    "        \n",
    "    for key in targets_flat: \n",
    "        targets_flat[key] = np.array(targets_flat[key])[indices]\n",
    "\n",
    "    return inputs_flat, targets_flat\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentations \n",
    "- Lightness\n",
    "- Hue \n",
    "- Gaussian Blur\n",
    "- Shadows \n",
    "- Rain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load driving logs\n",
    "- Loads data from all csv file in a lits of folders \n",
    "- Stores the data in a input dictionary and a target dictionary \n",
    "- Normalizes and coverts data to correct format \n",
    "- Does not use side cameraes for lane change data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def get_path(episode_path, image_path):\n",
    "    return str(episode_path / Path(\"images\") / image_path.split(\"/\")[-1])\n",
    "\n",
    "def load_driving_logs(dataset_folders, steering_correction):\n",
    "    \"\"\" \n",
    "    input: \n",
    "        dataset_folders: list of paths to folders to load data from \n",
    "        steering_correction: float, adjusts steering angles of forward facing side cameras \n",
    "        \n",
    "    Loads data from all csv file in a lits of folders \n",
    "    Stores the data in a input dictionary and a target dictionary \n",
    "    Normalizes and coverts data to correct format \n",
    "    Does not use side cameraes for lane change data\n",
    "    \n",
    "    return: \n",
    "        inputs: dictionary of all input data (image paths, info signals, HLCs, control signals)\n",
    "        targets: dictionary of all target data (steer, throttle)\n",
    "    \"\"\"\n",
    "    img_paths_center = []\n",
    "    \n",
    "    \n",
    "    inputs = create_input_dict()\n",
    "    \n",
    "    targets = create_target_dict()\n",
    "    # Loads data \n",
    "    for folder in dataset_folders:\n",
    "        folder_path = Path(\"dataset\") / folder\n",
    "        for episode in glob(str(folder_path / \"*\")):\n",
    "            \n",
    "            \n",
    "            temp_forward = {\"center\": [], \"left\": [], \"right\": []}\n",
    "            temp_hlcs =  {\"center\": [], \"left\": [], \"right\": []}\n",
    "\n",
    "            temp_steer =  {\"center\": [], \"left\": [], \"right\": []}\n",
    "            temp_throttle = {\"center\": [], \"left\": [], \"right\": []}\n",
    "                \n",
    "            episode_path = Path(episode)\n",
    "\n",
    "            df = pd.read_csv(str(episode_path / \"driving_log.csv\"))\n",
    "            for index, row in df.iterrows():\n",
    "                if index == 0: \n",
    "                    continue \n",
    "                                                        \n",
    "                steer = row[\"angle\"]\n",
    "                throttle = row[\"speed\"]\n",
    "                                \n",
    "                hlc = row[\"high_level_command\"]\n",
    "                #if hlc == 0 or hlc == -1:\n",
    "                #    hlc = 4\n",
    "                hlc = get_hlc_one_hot(hlc)\n",
    "                \n",
    "                temp_forward[\"center\"].append(get_path(episode_path, row[\"image_path\"]))              \n",
    "                temp_steer[\"center\"].append(steer)\n",
    "                temp_throttle[\"center\"].append(throttle)\n",
    "                temp_hlcs[\"center\"].append(hlc)\n",
    "                                \n",
    "                    \n",
    "            inputs[\"forward_imgs\"].append(temp_forward[\"center\"])\n",
    "                        \n",
    "            inputs[\"hlcs\"].append(temp_hlcs[\"center\"])\n",
    "                        \n",
    "            targets[\"steer\"].append(temp_steer[\"center\"])\n",
    "            \n",
    "            targets[\"throttle\"].append(temp_throttle[\"center\"])\n",
    "\n",
    "\n",
    "    print(\"Done, {:d} episode(s) loaded.\".format(len(inputs[\"forward_imgs\"])))\n",
    "\n",
    "    return(inputs, targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_data(dist, title=\"\"):\n",
    "    \"\"\" Plots distribution of HLC, speed and traffic lights \"\"\"\n",
    "    print(\"Plotting...\")\n",
    "    tot_num = 0\n",
    "    \n",
    "    fig = plt.figure(figsize=(16,6))\n",
    "\n",
    "    # HLC\n",
    "    labels =[\"Left\", \"Right\", \"Straight\"]\n",
    "    sizes = [dist[\"hlc\"][\"left\"], dist[\"hlc\"][\"right\"], dist[\"hlc\"][\"straight\"]]\n",
    "    \n",
    "    ax1 = fig.add_subplot(1,4,1)\n",
    "    wedges, texts, autotexts = ax1.pie(sizes, autopct='%.1f%%')\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "\n",
    "    ax1.set_title(\"Distrubution of HLC\")\n",
    "    ax1.legend(wedges,labels, loc=\"best\")\n",
    "    ax1.axis('equal')\n",
    "    \n",
    "\n",
    "    # Speed\n",
    "    labels =[\"Low Speed\", \"High Speed\"]\n",
    "    sizes = [dist[\"speed\"][\"low\"], dist[\"speed\"][\"high\"]]\n",
    "    \n",
    "    ax2 = fig.add_subplot(1,4,2)\n",
    "    wedges, texts, autotexts = ax2.pie(sizes, autopct='%.1f%%')\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    ax2.set_title(\"Distrubution of speed\")\n",
    "    ax2.legend(wedges,labels, loc=\"best\")\n",
    "    ax2.axis('equal')\n",
    "\n",
    "    # Steering\n",
    "    labels =[\"Left\", \"Straight\", \"Right\"]\n",
    "    print(\"Dist: \", dist)\n",
    "    sizes = [dist[\"steering\"][\"left\"], dist[\"steering\"][\"straight\"], dist[\"steering\"][\"right\"]]\n",
    "    \n",
    "    ax2 = fig.add_subplot(1,4,3)\n",
    "    wedges, texts, autotexts = ax2.pie(sizes, autopct='%.1f%%')\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    ax2.set_title(\"Distrubution of steering angle\")\n",
    "    ax2.legend(wedges,labels, loc=\"best\")\n",
    "    ax2.axis('equal')\n",
    "    \n",
    "\n",
    "    fig.suptitle(title + \": \" + str(tot_num) + \" sequences\", fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance Data \n",
    "\n",
    "- Get distribution of HLC, speed, speed limit, and traffic lights \n",
    "- Balance data for LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_dist(inputs, targets): \n",
    "    \"\"\" \n",
    "        input:\n",
    "            inputs: dictionary of input data \n",
    "            targets: dictionary of result data\n",
    "                \n",
    "        return: \n",
    "            dist: dictionary of distributions for HLC, speed, angle \n",
    "    \"\"\"\n",
    "    dist = {\n",
    "        \"hlc\": {\n",
    "            \"left\": 0,\n",
    "            \"right\": 0, \n",
    "            \"straight\": 0,\n",
    "        },\n",
    "        \"steering\": {\n",
    "            \"left\": 0, # More than 20 deg left\n",
    "            \"right\": 0, # More than 20 deg right\n",
    "            \"straight\": 0 # Between left and right\n",
    "        },\n",
    "        \"speed\": {\n",
    "            \"high\": 0, #More than 0.5m/s\n",
    "            \"low\": 0 #Less than 0.5 m/s\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    #print(\"getting dist\", inputs, targets)\n",
    "    # HLC distribution\n",
    "    for hlcs in inputs[\"hlcs\"]: \n",
    "        \n",
    "        # Iterate over all HLC in sequence \n",
    "        for hlc in hlcs: \n",
    "            hlc_value = np.argmax(hlc)\n",
    "            if hlc_value == 0:\n",
    "                dist[\"hlc\"][\"left\"] += 1\n",
    "            elif hlc_value == 1:\n",
    "                dist[\"hlc\"][\"straight\"] += 1\n",
    "            elif hlc_value == 2:\n",
    "                dist[\"hlc\"][\"right\"] += 1\n",
    "\n",
    "            \n",
    "    # Speed distribution \n",
    "    for speed in targets[\"throttle\"]: \n",
    "        if speed > 0.5:\n",
    "            dist[\"speed\"][\"high\"] +=1 \n",
    "        else: \n",
    "            dist[\"speed\"][\"low\"] +=1\n",
    "            \n",
    "            \n",
    "    # Steering distribution \n",
    "    for angle in targets[\"steer\"]: \n",
    "        if angle < -0.1:\n",
    "            dist[\"steering\"][\"left\"] +=1 \n",
    "        elif angle > 0.1:\n",
    "            dist[\"steering\"][\"right\"] += 1\n",
    "        else: \n",
    "            dist[\"steering\"][\"straight\"] +=1         \n",
    "\n",
    "    return dist   \n",
    "    \n",
    "\n",
    "def balance_steering_angle(inputs, targets, dist, target_straight_fraction): \n",
    "    \"\"\" Balance steer angle such that target fraction is correct \"\"\"\n",
    "    \n",
    "    # Find the steering with least amount of values\n",
    "    least_vals = min(dist[\"steering\"][\"straight\"], dist[\"steering\"][\"left\"], dist[\"steering\"][\"right\"])\n",
    "    \n",
    "    inputs_bal = create_input_dict()\n",
    "    targets_bal = create_target_dict() \n",
    "        \n",
    "    left_count = 0\n",
    "    right_count = 0\n",
    "    forward_count = 0\n",
    "    \n",
    "    for i in range(len(inputs[\"hlcs\"])):\n",
    "        angle = targets[\"steer\"][i]\n",
    "        is_left = angle < -0.1\n",
    "        is_right = angle > 0.1\n",
    "        \n",
    "        if is_left and left_count >= least_vals:\n",
    "            continue\n",
    "        \n",
    "        elif is_right and right_count >= least_vals:\n",
    "            continue\n",
    "        \n",
    "        elif not is_left and not is_right and forward_count >= least_vals:\n",
    "            continue\n",
    "            \n",
    "        if is_left:\n",
    "            left_count += 1\n",
    "        elif is_right:\n",
    "            right_count += 1\n",
    "        else:\n",
    "            forward_count += 1\n",
    "        \n",
    "        # Keep \n",
    "        for key in inputs_bal: \n",
    "            inputs_bal[key].append(inputs[key][i])\n",
    "        for key in targets_bal: \n",
    "            targets_bal[key].append(targets[key][i])\n",
    "\n",
    "    return inputs_bal, targets_bal \n",
    "\n",
    "\n",
    "def balance_data_lstm(inputs, targets, straight_angle_frac=0.2,debug=False, drop=False):\n",
    "    \"\"\" Balance dataset for LSTM data \"\"\"\n",
    "    print(\"Balancing data:\")\n",
    "    \n",
    "    # Get distribution \n",
    "    dist = get_dist(inputs, targets)\n",
    "    print(\"dist: \", dist)\n",
    "    \n",
    "    inputs_bal = inputs\n",
    "    targets_bal = targets\n",
    "    dist_bal = dist \n",
    "    \n",
    "    # Balance steering angle \n",
    "    print(\"   - Balancing steering angle\")\n",
    "    inputs_bal, targets_bal = balance_steering_angle(inputs_bal, targets_bal, dist_bal, straight_angle_frac)\n",
    "    dist_bal = get_dist(inputs_bal, targets_bal)\n",
    "    print(\"dist_bal: \", dist_bal)\n",
    "    \n",
    "\n",
    "    # Shuffle\n",
    "    inputs_bal, targets_bal = shuffle_data(inputs_bal, targets_bal)\n",
    "    \n",
    "    if drop: \n",
    "        inputs_bal, targets_bal = drop_follow_lane(inputs_bal,targets_bal, 3)\n",
    "        \n",
    "    dist_bal = get_dist(inputs_bal, targets_bal)\n",
    "        \n",
    "    return inputs_bal, targets_bal, dist_bal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop random data \n",
    "Randomly drop data that is follow lane without brake values or turns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def drop_follow_lane(inputs, targets, keep_every): \n",
    "    print(\"Dropping...\")\n",
    "    \n",
    "    inputs_dropped = create_input_dict()\n",
    "    targets_dropped = create_target_dict() \n",
    "    \n",
    "\n",
    "    for i in range(len(inputs[\"forward_imgs\"])): \n",
    "        \n",
    "        \n",
    "        follow_lane = True \n",
    "        # Iterate over all HLC in sequence \n",
    "        for hlc in inputs[\"hlcs\"][i]: \n",
    "            hlc_value = np.argmax(hlc) + 1 \n",
    "            # Only mark sequence as follow lane if all hlcs are follow lane  \n",
    "            if hlc_value != 4:\n",
    "                follow_lane = False \n",
    "                break\n",
    "                \n",
    "        steer = targets[\"steer\"][i]\n",
    "        \n",
    "        # Only drop if FOLLOW_LANE, no brake, and no turn\n",
    "        if follow_lane and abs(steer)<0.1:  \n",
    "            # Drop based on drop_rate \n",
    "            \n",
    "            if np.random.randint(keep_every) != 0: \n",
    "                continue\n",
    "\n",
    "        # Keep \n",
    "        for key in inputs_dropped: \n",
    "            inputs_dropped[key].append(inputs[key][i])\n",
    "        for key in targets_dropped: \n",
    "            targets_dropped[key].append(targets[key][i])\n",
    "\n",
    "    return inputs_dropped, targets_dropped \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_episode_sequences(data, sampling_interval, seq_length):\n",
    "    sequences = []\n",
    "    slices = []\n",
    "    for o in range(sampling_interval+1):\n",
    "        slices.append(data[o::sampling_interval+1])\n",
    "    for s in slices:\n",
    "        for o in range(0,len(s)):\n",
    "            if o + seq_length <= len(s):\n",
    "                sequences.append(s[o:o+seq_length])\n",
    "    return sequences\n",
    "\n",
    "def prepare_dataset_lstm(inputs, targets, sampling_interval, seq_length):\n",
    "    \n",
    "    inputs_flat = create_input_dict()\n",
    "    targets_flat = create_target_dict()\n",
    "    \n",
    "    for e in range(len(inputs[\"forward_imgs\"])):\n",
    "        [inputs_flat[\"forward_imgs\"].append(sequence) for sequence in get_episode_sequences(inputs[\"forward_imgs\"][e],sampling_interval, seq_length)]\n",
    "        [inputs_flat[\"hlcs\"].append(sequence) for sequence in get_episode_sequences(inputs[\"hlcs\"][e],sampling_interval, seq_length)]\n",
    "        [targets_flat[\"steer\"].append(sequence[-1]) for sequence in get_episode_sequences(targets[\"steer\"][e],sampling_interval, seq_length)]\n",
    "        [targets_flat[\"throttle\"].append(sequence[-1]) for sequence in get_episode_sequences(targets[\"throttle\"][e],sampling_interval, seq_length)]\n",
    "\n",
    "    \n",
    "    # Shuffle\n",
    "    inputs_flat, targets_flat = shuffle_data(inputs_flat, targets_flat)\n",
    "    \n",
    "    return(inputs_flat, targets_flat)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_segmentation_model():\n",
    "    segmentation_model = model_from_checkpoint_path(checkpoints_path)\n",
    "\n",
    "    x = segmentation_model.layers[-4].output\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # Explicitly define new model input and output by slicing out old model layers\n",
    "    model_new = Model(inputs=segmentation_model.layers[0].input, \n",
    "                      outputs=x)\n",
    "\n",
    "    for layer in model_new.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    return model_new\n",
    "\n",
    "def get_lstm_model(seq_length, sine_steering=False, print_summary=True):\n",
    "    \n",
    "    forward_image_input = Input(shape=(seq_length, 473, 473, 3), name=\"forward_image_input\")\n",
    "    hlc_input = Input(shape=(seq_length,3), name=\"hlc_input\")\n",
    "    \n",
    "    segmentation_model = get_segmentation_model()\n",
    "    segmentation_output = TimeDistributed(segmentation_model)(forward_image_input)\n",
    "\n",
    "    #x = TimeDistributed(Cropping2D(cropping=((50,0),(0,0))))(forward_image_input)\n",
    "    #x = TimeDistributed(Conv2D(24,(5,5),strides=(2,2), activation=\"relu\"))(x)\n",
    "    #x = TimeDistributed(Conv2D(36,(5,5),strides=(2,2), activation=\"relu\"))(x)\n",
    "    #x = TimeDistributed(Conv2D(48,(5,5),strides=(2,2), activation=\"relu\"))(x)\n",
    "    #x = TimeDistributed(Conv2D(64,(3,3),strides=(2,2), activation=\"relu\"))(x)\n",
    "    #x = TimeDistributed(Conv2D(64,(3,3), activation=\"relu\"))(x)\n",
    "    #x = TimeDistributed(Conv2D(64,(3,3), activation=\"relu\"))(x)\n",
    "    #conv_output = TimeDistributed(Flatten())(x)\n",
    "\n",
    "    #x = concatenate([conv_output, hlc_input])\n",
    "    x = concatenate([segmentation_output, hlc_input])\n",
    "    \n",
    "    x = TimeDistributed(Dense(100, activation=\"relu\"))(x)\n",
    "    x = CuDNNLSTM(10, return_sequences = False)(x)\n",
    "    steer_dim = 1 if not sine_steering else 10\n",
    "    steer_pred = Dense(steer_dim, activation=\"tanh\", name=\"steer_pred\")(x)\n",
    "    \"\"\"\n",
    "    segmentation_output = TimeDistributed(segmentation_model)(forward_image_input)\n",
    "    #x = TimeDistributed(Cropping2D(cropping=((50,0),(0,0))))(forward_image_input)\n",
    "    #x = TimeDistributed(Conv2D(24,(5,5),strides=(2,2), activation=\"relu\"))(x)\n",
    "    #x = TimeDistributed(Conv2D(36,(5,5),strides=(2,2), activation=\"relu\"))(x)\n",
    "    #x = TimeDistributed(Conv2D(48,(5,5),strides=(2,2), activation=\"relu\"))(x)\n",
    "    #x = TimeDistributed(Conv2D(64,(3,3),strides=(2,2), activation=\"relu\"))(x)\n",
    "    #x = TimeDistributed(Conv2D(64,(3,3), activation=\"relu\"))(x)\n",
    "    #x = TimeDistributed(Conv2D(64,(3,3), activation=\"relu\"))(x)\n",
    "    #conv_output = TimeDistributed(Flatten())(x)\n",
    "\n",
    "    #x = concatenate([conv_output, hlc_input])\n",
    "    x = concatenate([segmentation_output, hlc_input])\n",
    "    \n",
    "    x = TimeDistributed(Dense(100, activation=\"relu\"))(x)\n",
    "    x = CuDNNLSTM(10, return_sequences = False)(x)\n",
    "    \"\"\"\n",
    "    throtte_pred = Dense(1, name=\"throttle_pred\")(x)\n",
    "    \n",
    "    model = Model(inputs=[forward_image_input, hlc_input], outputs=[steer_pred,throtte_pred])\n",
    "\n",
    "    if print_summary: \n",
    "        model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "#mol = get_lstm_model(10)\n",
    "# get_segmentation_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class generator(Sequence):\n",
    "    def __init__(self, inputs, targets, batch_size, validation=False, sine_steering=False):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.batch_size = batch_size\n",
    "        self.validation = validation\n",
    "        self.sine_steering = sine_steering\n",
    "        \n",
    "        random.seed() \n",
    "        # Convert to np array\n",
    "        for key in inputs: \n",
    "            inputs[key] = np.array(self.inputs[key])\n",
    "\n",
    "        for key in targets: \n",
    "            targets[key] = np.array(self.targets[key])\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.inputs[\"forward_imgs\"]) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subset = np.arange(idx * self.batch_size,min((idx + 1) * self.batch_size,len(self.inputs[\"forward_imgs\"])))\n",
    "        forward_imgs = []\n",
    "        read_images = [[cv2.resize(cv2.imread(path), None, fx=0.5, fy=0.5) for path in seq] for seq in self.inputs[\"forward_imgs\"][subset]]\n",
    "        steer_pred = self.targets[\"steer\"][subset]\n",
    "        if self.sine_steering:\n",
    "            steer_pred = np.array([sine_encode(steer) for steer in steer_pred])\n",
    "        if not self.validation: \n",
    "            for seq in read_images:\n",
    "                forward_imgs.append([cv2.cvtColor(image,cv2.COLOR_BGR2LAB) for image in seq])\n",
    "        else:\n",
    "            for seq in read_images:\n",
    "                forward_imgs.append([cv2.cvtColor(image,cv2.COLOR_BGR2LAB) for image in seq])\n",
    "\n",
    "        forward_imgs = np.array(forward_imgs)/255.0 - 0.5\n",
    "        return {\n",
    "            \"forward_image_input\": np.array(forward_imgs), \n",
    "            \"hlc_input\": self.inputs[\"hlcs\"][subset]\n",
    "        }, {\n",
    "            \"steer_pred\": steer_pred,\n",
    "            \"throttle_pred\": self.targets[\"throttle\"][subset],\n",
    "        }\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "val_split = 0.8\n",
    "adjust_hlc = False \n",
    "balance_data = False \n",
    "\n",
    "\n",
    "epochs_list = [100]\n",
    "dataset_folders_lists = [[\"triangular_noise_glos_train\"]]\n",
    "\"\"\"dataset_folders_lists = [([\n",
    "    \"etron/Town01/ClientAP/no_cars_no_rain\",\n",
    "    \"etron/Town01/ClientAP/no_cars_no_rain_noise15\",\n",
    "    \"etron/Town01/ClientAP/no_cars_rain_noise10\",\n",
    "    \"etron/Town01/ClientAP/cars_no_rain_noise15\",\n",
    "    \"etron/Town01/ClientAP/cars_rain_noise10\",    \n",
    "], [\n",
    "    \"etron/Town04/ClientAP/no_cars_no_rain\", \n",
    "    \"etron/Town04/ClientAP/no_cars_rain_noise5\", \n",
    "    \"etron/Town04/ClientAP/cars_no_rain_noise5\", \n",
    "    \"etron/Town04/ClientAP/cars_rain_noise5\", \n",
    "    \"etron/Town01/ClientAP/brake_all_weather_noise15\",\n",
    "    \"etron/Town01/ClientAP/brake_all_weather\",\n",
    "])]\"\"\"\n",
    "\n",
    "\n",
    "steering_corrections = [0.05]\n",
    "\n",
    "batch_sizes = [32]\n",
    "\n",
    "sampling_intervals = [3]\n",
    "\n",
    "seq_lengths = [10]\n",
    "\n",
    "sine_steering_list = [False]\n",
    "\n",
    "balance_data_list = [False]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of model test: test_model\n",
      "Done, 1 episode(s) loaded.\n",
      "Initiate training loop with the following parameters:\n",
      "---\n",
      "epochs:\t\t\t100\n",
      "dataset folders:\t['triangular_noise_glos_train']\n",
      "steering correction:\t0.05\n",
      "batch size:\t\t32\n",
      "balance:\t\tFalse\n",
      "sine_steer:\t\tFalse\n",
      "sampling interval:\t3\n",
      "seq lenght: \t\t10\n",
      "\n",
      "\n",
      "---\n",
      "Training set size: 11524\n",
      "Validation set size: 2882\n",
      "WARNING:tensorflow:From /home/audun/anaconda3/envs/segmentation-prediction/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/audun/anaconda3/envs/segmentation-prediction/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "loaded weights  /home/audun/master-thesis-code/training/psp_checkpoints_best/pspnet_50_three.0\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "forward_image_input (InputLayer (None, 10, 473, 473, 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 10, 10800)    46765763    forward_image_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "hlc_input (InputLayer)          (None, 10, 3)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 10, 10803)    0           time_distributed_1[0][0]         \n",
      "                                                                 hlc_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 10, 100)      1080400     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)        (None, 10)           4480        time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "steer_pred (Dense)              (None, 1)            11          cu_dnnlstm_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "throttle_pred (Dense)           (None, 1)            11          cu_dnnlstm_1[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 47,850,665\n",
      "Trainable params: 1,084,902\n",
      "Non-trainable params: 46,765,763\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /home/audun/anaconda3/envs/segmentation-prediction/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "CuDNNLSTMparameter_permutations = itertools.product(epochs_list, \n",
    "                                           dataset_folders_lists, \n",
    "                                           steering_corrections, \n",
    "                                           batch_sizes,\n",
    "                                           sampling_intervals,\n",
    "                                           seq_lengths,\n",
    "                                           sine_steering_list,\n",
    "                                           balance_data_list)\n",
    "\n",
    "# Train a new model for each parameter permutation, and save the best models\n",
    "model_name = input(\"Name of model test: \").strip()\n",
    "#balance_data = True if input(\"Balance data y/[n]: \").lower() == \"y\" else False \n",
    "#drop_data = True if input(\"Drop data y/[n]: \").lower() == \"y\" else False \n",
    "\n",
    "parameter_permutations_list = []#[p for p in parameter_permutations]\n",
    "# glos_cycle_full_folders = [\"glos_cycle_track_wet_clouded\", \"glos_cycle_noise_mini\"]\n",
    "glos_cycle_full_folders = [\"triangular_noise_glos_train\"]\n",
    "parameter_permutations_list.append([epochs_list[0], \n",
    "                                glos_cycle_full_folders, \n",
    "                                steering_corrections[0], \n",
    "                                batch_sizes[0],\n",
    "                                sampling_intervals[0],\n",
    "                                seq_lengths[0],\n",
    "                                False,\n",
    "                                False])\n",
    "\n",
    "parameter_permutations_list.append([epochs_list[0], \n",
    "                                glos_cycle_full_folders, \n",
    "                                steering_corrections[0], \n",
    "                                batch_sizes[0],\n",
    "                                sampling_intervals[0],\n",
    "                                seq_lengths[0],\n",
    "                                True,\n",
    "                                False])\n",
    "\n",
    "\n",
    "\n",
    "for parameters in parameter_permutations_list:\n",
    "    # Get parameters\n",
    "    epochs, dataset_folders, steering_correction, batch_size, sampling_interval, seq_length, sine_steering, balance_data = parameters\n",
    "    parameters_string = (\"epochs:\\t\\t\\t{}\\ndataset folders:\\t{}\\nsteering correction:\\t{}\\nbatch size:\\t\\t{}\\nbalance:\\t\\t{}\\nsine_steer:\\t\\t{}\\nsampling interval:\\t{}\\nseq lenght: \\t\\t{}\\n\\n\"\n",
    "                         .format(epochs, str(dataset_folders), steering_correction, batch_size, balance_data, sine_steering, sampling_interval, seq_length))\n",
    "    \n",
    "    \n",
    "    #town1_dataset_folders, town4_dataset_folders = dataset_folders\n",
    "\n",
    "    # Prepare for logging\n",
    "    timestamp = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.localtime(time.time()))\n",
    "    path = Path('models') / model_name / timestamp\n",
    "    if not os.path.exists(str(path)):\n",
    "        os.makedirs(str(path))\n",
    "        \n",
    "    # Save parmeters to disk\n",
    "    with open(str(path / \"parameters.txt\"), \"w\") as text_file:\n",
    "        text_file.write(parameters_string)\n",
    "    \n",
    "    # Save config file to disk \n",
    "    model_name = \"lstm\"\n",
    "    config = configparser.ConfigParser()\n",
    "    config[\"ModelConfig\"] = {'Model': model_name,'sequence_length': seq_length,'sampling_interval': sampling_interval}\n",
    "    with open(str(path/'config.ini'), 'w') as configfile:\n",
    "        config.write(configfile)\n",
    "    \n",
    "    # Load drive logs and paths\n",
    "    inputs, targets = load_driving_logs(dataset_folders, steering_correction)\n",
    "    inputs_flat, targets_flat = prepare_dataset_lstm(inputs, targets, sampling_interval, seq_length)\n",
    "\n",
    "    # Balance data \n",
    "    if balance_data:\n",
    "        # Plot data before balancing \n",
    "        title_before = \"Before Balancing\"\n",
    "        #print(\"inputs_flat\", inputs_flat)\n",
    "        plot_data(get_dist(inputs_flat, targets_flat), title=title_before)\n",
    "\n",
    "        # Balance data \n",
    "        inputs_flat, targets_flat, dist_bal1 = balance_data_lstm(inputs_flat, targets_flat)\n",
    "\n",
    "        # Plot data after balancing \n",
    "        title_after = \"After Balancing\"\n",
    "        plot_data(dist_bal1, title=title_after)\n",
    "\n",
    "\n",
    "    inputs_flat_dict = create_input_dict() \n",
    "    targets_flat_dict = create_target_dict() \n",
    "\n",
    "    for key in inputs_flat: \n",
    "        inputs_flat_dict[key] = inputs_flat[key] \n",
    "\n",
    "    for key in targets_flat: \n",
    "        targets_flat_dict[key] = targets_flat[key]\n",
    "                \n",
    "    # Shuffle data \n",
    "    inputs_flat, targets_flat = shuffle_data(inputs_flat, targets_flat)           \n",
    "    \n",
    "    # Split into val and train\n",
    "    split_pos = int(val_split*len(inputs_flat[\"forward_imgs\"]))\n",
    "    inputs_train, inputs_val = split_dict(inputs_flat, split_pos)\n",
    "    targets_train, targets_val = split_dict(targets_flat, split_pos)\n",
    "\n",
    "    # Print training info \n",
    "    train_num = len(inputs_train[\"forward_imgs\"])\n",
    "    val_num = len(inputs_val[\"forward_imgs\"])\n",
    "    print(\"Initiate training loop with the following parameters:\")\n",
    "    print(\"---\")\n",
    "    print(parameters_string)\n",
    "    print(\"---\")\n",
    "    print(\"Training set size: \" + str(train_num))\n",
    "    print(\"Validation set size: \" + str(val_num))\n",
    "\n",
    "    # Get model\n",
    "    model = get_lstm_model(seq_length, print_summary=False, sine_steering=sine_steering)\n",
    "\n",
    "    \n",
    "    # Compile model \n",
    "    model.compile(loss=[steer_loss(), mean_squared_error] , optimizer=Adam())\n",
    "\n",
    "    checkpoint_val = ModelCheckpoint(str(path / ('{epoch:02d}_s{val_steer_pred_loss:.4f}_t{val_throttle_pred_loss:.4f}.h5')), monitor='val_loss', verbose=1, save_best_only=False,mode=\"min\")\n",
    "    \n",
    "    # Create image of model architecture \n",
    "    #plot_model(model, str(path/'model.png'))\n",
    "    \n",
    "    steps = int(train_num/batch_size)\n",
    "    steps_val = int(val_num/batch_size)\n",
    "     \n",
    "        \n",
    "    # Define early stopping params\n",
    "    es = EarlyStopping(monitor='val_steer_pred_loss', mode='min', verbose=1, patience=8)\n",
    "    \n",
    "    # Train model\n",
    "    print(model.summary())\n",
    "    history_object = model.fit_generator(\n",
    "        generator(inputs_train, targets_train, batch_size, sine_steering=sine_steering),\n",
    "        validation_data=generator(inputs_val, targets_val, batch_size, validation=True, sine_steering=sine_steering),\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        callbacks=[checkpoint_val, es],\n",
    "        steps_per_epoch=steps,\n",
    "        validation_steps = steps_val,\n",
    "        use_multiprocessing = True,\n",
    "        workers=10\n",
    "    )\n",
    "    \n",
    "    # Prepare plot and save it to disk\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    print(\"History\", history_object.history)\n",
    "    ax.plot(history_object.history['steer_pred_loss'], color=\"blue\")\n",
    "    ax.plot(history_object.history['val_steer_pred_loss'], color=\"blue\", linestyle=\"--\")\n",
    "    ax.plot(history_object.history['throttle_pred_loss'], color=\"green\")\n",
    "    ax.plot(history_object.history['val_throttle_pred_loss'], color=\"green\", linestyle=\"--\")\n",
    "\n",
    "    ax.set_title(\"Mean squared loss of: throttle and steer\")\n",
    "    ax.set_xlabel(\"epochs\")\n",
    "    ax.set_ylabel(\"mse\")\n",
    "\n",
    "    lgd = ax.legend(['steer loss', \n",
    "               'steer validation loss',\n",
    "               'throttle loss', \n",
    "               'throttle validation loss'], bbox_to_anchor=(1.1, 1.05))\n",
    "    \n",
    "    plt.show()\n",
    "    fig.savefig(str(path / 'loss.png'), bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "    print('\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
